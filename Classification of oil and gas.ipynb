{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fe1818-9d07-4758-9676-875b385ee329",
   "metadata": {},
   "source": [
    "# Задача классификации на Kaggle с помощью логистической регрессии. Определение типа месторождения нефти и газа\n",
    "\n",
    "### Цель проекта\n",
    "\n",
    "Необходимо реализовать решение задачи классификации нефтегазовых месторождений: определить, относится ли месторождение к типу **onshore** (на суше) или **offshore** (на море) по совокупности географических, геологических и технологических параметров\n",
    "\n",
    "Этапы работы:\n",
    "\n",
    "* Провести исследование и первичный анализ данных (EDA)\n",
    "* Выполнить предобработку данных (обработка пропусков, аномалий, кодирование категориальных признаков)\n",
    "* При необходимости реализовать feature engineering\n",
    "* Подобрать и проанализировать значимые признаки\n",
    "* Обучить несколько моделей классификации и сравнить их качество\n",
    "* Выполнить подбор гиперпараметров для ключевых моделей\n",
    "* Выбрать лучшую модель и обосновать выбор\n",
    "* Построить предсказания для тестового набора данных\n",
    "\n",
    "Качество решения оценивается по метрике **F1-score**\n",
    "Финальный ответ оформляется в виде файла `submission.csv` с двумя колонками:\n",
    "\n",
    "* `index` — индекс строки тестового набора\n",
    "* `Onshore/Offshore` — предсказанный класс (0 — OFFSHORE, 1 — ONSHORE, 2 — ONSHORE-OFFSHORE)\n",
    "\n",
    "### Описание данных\n",
    "* `train.csv` — тренировочный набор данных c целевой переменной (309 строк). [Скачать датасет](https://disk.yandex.ru/d/-drOTkpk1phipg)\n",
    "\n",
    "* `test.csv` — тестовый набор данных без целевой переменной (133 строки). [Скачать датасет](https://disk.yandex.ru/d/izxHxDF5D17FYg)\n",
    "\n",
    "Каждое месторождение описывается **19 признаками**:\n",
    "\n",
    "* `Field name` — название месторождения;\n",
    "* `Reservoir unit` — юнит (подразделение) месторождения;\n",
    "* `Country` — страна расположения месторождения;\n",
    "* `Region` — регион расположения;\n",
    "* `Basin name` — название осадочного бассейна;\n",
    "* `Tectonic regime` — тектонический режим;\n",
    "* `Latitude` — широта месторождения;\n",
    "* `Longitude` — долгота месторождения;\n",
    "* `Operator company` — оператор / компания-разработчик;\n",
    "* **`Onshore or offshore` — целевая переменная** (0 — OFFSHORE, 1 — ONSHORE, 2 — ONSHORE-OFFSHORE);\n",
    "* `Hydrocarbon type (main)` — основной тип углеводорода (нефть, газ, конденсат и т.п.);\n",
    "* `Reservoir status (current)` — текущий статус месторождения (в разработке, законсервировано и т.д.);\n",
    "* `Structural setting` — структурные особенности месторождения;\n",
    "* `Depth (top reservoir ft TVD)` — глубина залегания верхней границы коллектора (футы, TVD);\n",
    "* `Reservoir period` — геологический/литологический период формирования коллектора;\n",
    "* `Lithology (main)` — основная литология (тип породы);\n",
    "* `Thickness (gross average ft)` — средняя общая мощность (толщина) пласта, ft;\n",
    "* `Thickness (net pay average ft)` — средняя эффективная мощность пласта, ft;\n",
    "* `Porosity (matrix average)` — средняя пористость коллектора;\n",
    "* `Permeability (air average mD)` — средняя проницаемость (в миллидарси, mD).\n",
    "\n",
    "На основе этих признаков необходимо построить модель, способную максимально точно классифицировать тип месторождения по признаку **Onshore/Offshore**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11deef78-ad98-40d3-9c95-f2e45d0617fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd116171-a1c8-4393-9756-89e87d557bfa",
   "metadata": {},
   "source": [
    "## 1. Загрузка датасетов по API с Яндекс.Диска."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c92739-9234-4465-9549-c8b2460b4f5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1344\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1344\u001b[0m     h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1345\u001b[0m               encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1336\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1382\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1381\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1091\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1094\u001b[0m \n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m-> 1035\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1477\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1475\u001b[0m     server_hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m-> 1477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mwrap_socket(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock,\n\u001b[0;32m   1478\u001b[0m                                       server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:455\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    450\u001b[0m                 do_handshake_on_connect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    451\u001b[0m                 suppress_ragged_eofs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    452\u001b[0m                 server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msslsocket_class\u001b[38;5;241m.\u001b[39m_create(\n\u001b[0;32m    456\u001b[0m         sock\u001b[38;5;241m=\u001b[39msock,\n\u001b[0;32m    457\u001b[0m         server_side\u001b[38;5;241m=\u001b[39mserver_side,\n\u001b[0;32m    458\u001b[0m         do_handshake_on_connect\u001b[38;5;241m=\u001b[39mdo_handshake_on_connect,\n\u001b[0;32m    459\u001b[0m         suppress_ragged_eofs\u001b[38;5;241m=\u001b[39msuppress_ragged_eofs,\n\u001b[0;32m    460\u001b[0m         server_hostname\u001b[38;5;241m=\u001b[39mserver_hostname,\n\u001b[0;32m    461\u001b[0m         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m         session\u001b[38;5;241m=\u001b[39msession\n\u001b[0;32m    463\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1041\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1319\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mdo_handshake()\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m train_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://disk.yandex.ru/d/izxHxDF5D17FYg\u001b[39m\u001b[38;5;124m\"\u001b[39m   \n\u001b[0;32m     11\u001b[0m test_url  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://disk.yandex.ru/d/-drOTkpk1phipg\u001b[39m\u001b[38;5;124m\"\u001b[39m   \n\u001b[1;32m---> 13\u001b[0m train \u001b[38;5;241m=\u001b[39m load_dataset_from_yadisk(train_url)\n\u001b[0;32m     14\u001b[0m test  \u001b[38;5;241m=\u001b[39m load_dataset_from_yadisk(test_url)\n\u001b[0;32m     15\u001b[0m train\u001b[38;5;241m.\u001b[39mshape, test\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mload_dataset_from_yadisk\u001b[1;34m(public_key)\u001b[0m\n\u001b[0;32m      4\u001b[0m resp\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m      5\u001b[0m download_url \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(download_url)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    729\u001b[0m     path_or_buf,\n\u001b[0;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[0;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m urlopen(req_info) \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[0;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, protocol, protocol \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    533\u001b[0m                           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_open(http\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mHTTPSConnection, req,\n\u001b[0;32m   1393\u001b[0m                         context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\urllib\\request.py:1347\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1344\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[0;32m   1345\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m-> 1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m   1348\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [WinError 10060] Попытка установить соединение была безуспешной, т.к. от другого компьютера за требуемое время не получен нужный отклик, или было разорвано уже установленное соединение из-за неверного отклика уже подключенного компьютера>"
     ]
    }
   ],
   "source": [
    "def load_dataset_from_yadisk(public_key: str) -> pd.DataFrame:\n",
    "    base_url = \"https://cloud-api.yandex.net/v1/disk/public/resources/download\"\n",
    "    resp = requests.get(base_url, params={\"public_key\": public_key})\n",
    "    resp.raise_for_status()\n",
    "    download_url = resp.json()[\"href\"]\n",
    "    df = pd.read_csv(download_url)\n",
    "    return df\n",
    "\n",
    "# Заранее разбиваем их на обучающую и тестовую выборки\n",
    "train_url = \"https://disk.yandex.ru/d/izxHxDF5D17FYg\"   \n",
    "test_url  = \"https://disk.yandex.ru/d/-drOTkpk1phipg\"   \n",
    "\n",
    "train = load_dataset_from_yadisk(train_url)\n",
    "test  = load_dataset_from_yadisk(test_url)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b530aa-e10c-412b-a879-1c8989ee49a6",
   "metadata": {},
   "source": [
    "## 2. Первичный EDA\n",
    "Смотрим структуру данных, типы, описательную статистику и пропуски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962fae43-f224-4015-8a19-c91c2c4efc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())\n",
    "print(train.info())\n",
    "print(\"\\nЧисловые признаки (описательная статистика):\")\n",
    "display(train.describe())\n",
    "print(\"\\nПропуски в train:\")\n",
    "print(train.isna().sum())\n",
    "print(\"\\nПропуски в test:\")\n",
    "print(test.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b04ab-2316-498c-9c4e-65119c02ae5b",
   "metadata": {},
   "source": [
    "По результатам EDA видим, что в следующих столбцах, для обоих выборок присутствуют пропуски: `Country`, `Region`, `Basin name`, `Latitude`, `Longitude`. Для логистической регрессии NaN недопустимы. Удалять строки нецелесообразно, т.к данных немного. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2715-76f8-462f-8cd2-b164b4cd98e8",
   "metadata": {},
   "source": [
    "## 3. Предобработка данных и feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05329a34-1905-43f4-8c6d-b922b0066c47",
   "metadata": {},
   "source": [
    "Применим следующие методы для заполнения пропусков:\n",
    "\n",
    "* для категориальных (`Country`, `Region`, `Basin name`) - отнесем в отдельную категорию `\"Unknown\"`. \n",
    "* для `Latitude`, `Longitude` — заполнм NaN-ы медианным занчением, т.к она устойчива к выбросам. Расчёт только по train исключает утечку информации из тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab681e5-d749-437a-a7cb-055b40e8ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Копии исходных данных\n",
    "train_prep = train.copy()\n",
    "test_prep = test.copy()\n",
    "\n",
    "# Заводим отдельную категорию \"Unknown\"\n",
    "geo_cat_cols = [\"Country\", \"Region\", \"Basin name\"]\n",
    "\n",
    "for col in geo_cat_cols:\n",
    "    train_prep[col] = train_prep[col].fillna(\"Unknown\")\n",
    "    test_prep[col] = test_prep[col].fillna(\"Unknown\")\n",
    "\n",
    "# Заполняем медианой по train, затем применяем к test\n",
    "geo_num_cols = [\"Latitude\", \"Longitude\"]\n",
    "\n",
    "for col in geo_num_cols:\n",
    "    median_val = train_prep[col].median()\n",
    "    train_prep[col] = train_prep[col].fillna(median_val)\n",
    "    test_prep[col] = test_prep[col].fillna(median_val)\n",
    "\n",
    "print(\"Пропуски в train после обработки:\")\n",
    "print(train_prep.isna().sum())\n",
    "print(\"\\nПропуски в test после обработки:\")\n",
    "print(test_prep.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dde6e-39d9-4443-a5f1-9be21e481c8a",
   "metadata": {},
   "source": [
    "**Кодировка целевой переменной в 0/1/2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42bba1-692c-4ef7-a063-30aef7773925",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"Onshore/Offshore\"\n",
    "label_map = {\"OFFSHORE\": 0, \"ONSHORE\": 1, \"ONSHORE-OFFSHORE\": 2}\n",
    "train_prep[\"target\"] = train_prep[target_col].map(label_map)\n",
    "\n",
    "# Список без целевой переменной\n",
    "feature_cols = [col for col in train_prep.columns if col not in [target_col, \"target\"]]\n",
    "\n",
    "X_full = train_prep[feature_cols]\n",
    "y_full = train_prep[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c8def-8518-4c38-8300-acd389091b6d",
   "metadata": {},
   "source": [
    " - **Разделим признаки на числовые и категориальные, чтобы в дальнейшем применять к ним разные типы преобразований**\n",
    "\n",
    " - **В рамках Feature engineering, добавим два новых признака**\n",
    "\n",
    "1) Коэффициент эффективной мощности пласта `net_to_gross`. Является распространенным показателем в нефтегазе при анлизе ГИС (геофизические исследования скважин). Отражает долю пласта, которая реально участвует в фильтрации. В отличие от двух отдельных толщин, отношение делает разные месторождения сопоставимыми между собой и может лучше улавливать различия между onshore и offshore объектами\n",
    "\n",
    "$$\n",
    "\\text{net\\_to\\_gross} =\n",
    "\\frac{\\text{Thickness (net pay average ft)}}{\\text{Thickness (gross average ft)}}\n",
    "$$\n",
    "\n",
    "2) Логарифм глубины `log_depth`. Логарифмирование сглаживает правостороннюю асимметрию распределения глубины и делает зависимость между глубиной и классом более удобной для обучения линейных моделей, как в нашем случае\n",
    "\n",
    "$$\n",
    "\\text{log\\_depth} = \\log\\bigl(\\text{Depth} + 1\\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b065c-a27f-4c76-a79f-ca25d45b80ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовый набор численных признаков\n",
    "numeric_features = [\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Depth\",\n",
    "    \"Thickness (gross average ft)\",\n",
    "    \"Thickness (net pay average ft)\",\n",
    "    \"Porosity\",\n",
    "    \"Permeability\",\n",
    "]\n",
    "X_full = X_full.copy()\n",
    "\n",
    "# Расчет новых признаков\n",
    "X_full[\"net_to_gross\"] = (\n",
    "    X_full[\"Thickness (net pay average ft)\"] / X_full[\"Thickness (gross average ft)\"]\n",
    ")\n",
    "X_full[\"log_depth\"] = np.log1p(X_full[\"Depth\"])\n",
    "numeric_features_extended = numeric_features + [\"net_to_gross\", \"log_depth\"]\n",
    "print(\"Числовые признаки:\", numeric_features_extended)\n",
    "\n",
    "# Список категориальных признаков\n",
    "categorical_features = [col for col in X_full.columns if col not in numeric_features_extended]\n",
    "print(\"Категориальные признаки:\", categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feeef62-f854-4edf-b82f-d0912382cd9a",
   "metadata": {},
   "source": [
    "**Разбиение на train и validation**\n",
    "\n",
    "Делим исходные данные на обучающую и валидационную выборки для оценки качества моделей и подбора гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fca2d2-10fe-4f76-91a4-463fc4b46fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_full,\n",
    "    y_full,\n",
    "    test_size=0.2,\n",
    "    stratify=y_full,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_valid.shape:\", X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d43a99-28cd-4da4-9385-c4bfd92041b7",
   "metadata": {},
   "source": [
    "Разбиение выборки (80%/20%) выполнено корректно. Из 309 объектов исходного датасета получено 247 наблюдений в обучающей выборке и 62 в валидационной, при одинаковом количестве признаков (21 столбец). Можем переходить к этапу обучения и сравнительного анализа моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549b3b1-103e-4819-ab20-8544488493f4",
   "metadata": {},
   "source": [
    "## 4. Pipeline предобработки для сравнения моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdb60f-cd62-47b8-8317-72b5c7217d37",
   "metadata": {},
   "source": [
    "Вместо `LabelEncoder`, который задаёт искусственный порядок категориям, мы используем связку **`ColumnTransformer` + `Pipeline`**, чтобы в едином конвейере **«предобработка → модель»** масштабировать числовые признаки (`StandardScaler`) и кодировать категориальные (`OneHotEncoder`) \n",
    "\n",
    "- Класс `ColumnTransformer` из `sklearn` позволяет применить эти преобразования сразу к группам столбцов (числовые и категориальные) и собрать их в единую матрицу признаков\n",
    "\n",
    "- Класс `Pipeline` из `sklearn` объединяет шаги предобработки и сам алгоритм машинного обучения в один объект. Снаружи он выглядит как обычная модель с методами `fit` и `predict`, а внутри по шагам:\n",
    "  1. На этапе `fit` обучает трансформации (масштабирование, One-Hot) только на тренировочных данных и затем обучает модель на уже преобразованных признаках;\n",
    "  2. На этапе `predict` автоматически применяет те же самые трансформации к новым данным и выдаёт предсказания\n",
    "\n",
    "Использование `Pipeline` удобно тем, что один и тот же конвейер можно передавать в `cross_val_score` и `GridSearchCV`. ТАк все шаги переобучаются внутри каждого фолда, что снижает риск утечки информации и упрощает код."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697894a2-0fef-4025-962d-a5f8884da18d",
   "metadata": {},
   "source": [
    "Также, задаём схему, которая собирается в `ColumnTransformer`, который затем используется внутри `Pipeline` как часть единого конвейера **«предобработка → модель»**\n",
    "\n",
    "- **Числовые признаки** стандартизируем с помощью `StandardScaler`, чтобы привести их к единому масштабу и упростить обучение моделей\n",
    "- **Категориальные признаки** кодируем через `OneHotEncoder`, чтобы превратить каждое категориальное значение в отдельный бинарный столбец и корректно обрабатывать новые категории при валидации и тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f943896-e2af-4bce-88d3-17ab92423b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пайплайн для числовых признаков (StandardScaler)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Пайплайн для категориальных признаков (OneHotEncoder)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Отдельно обрабатываем num и cat признаки\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features_extended),\n",
    "        (\"cat\", categorical_transformer, categorical_features),])\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1beb4-6cd4-4eee-bc69-3f9d053a4036",
   "metadata": {},
   "source": [
    "**Здесь задаём три базовые модели, которые будем сравнивать на одной и той же схеме предобработки, затем модели будут подключаться в пайплайн**\n",
    "\n",
    "- **LogisticRegression** - линейный базовый алгоритм, чувствительный к масштабу признаков (поэтому и применяли `StandardScaler`)\n",
    "- **RandomForestClassifier** — это «лес» из независимых деревьев решений, каждое из которых строится на своём подмножестве признаков и объектов\n",
    "- **GradientBoostingClassifier** — последовательная композиция слабых деревьев, где каждое следующее дерево «подправляет» ошибки предыдущих"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d9851-06aa-4ebf-b17b-4cd9d2d61999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Набор моделей \n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=42,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    ),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(\n",
    "        random_state=42)}\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f473c95-1786-4f65-ac75-57ba85bcfead",
   "metadata": {},
   "source": [
    "**Качество решений оценивается по **F1-score** - метрике для задач классификации, которая объединяет **точность** (precision) и **полноту** (recall)**\n",
    "\n",
    "Для класса \\(k\\):\n",
    "\n",
    "$$\n",
    "\\text{Precision}_k = \\frac{TP_k}{TP_k + FP_k}, \\quad\n",
    "\\text{Recall}_k = \\frac{TP_k}{TP_k + FN_k},\n",
    "$$\n",
    "\n",
    "$$\n",
    "F1_k = 2 \\cdot \\frac{\\text{Precision}_k \\cdot \\text{Recall}_k}{\\text{Precision}_k + \\text{Recall}_k}.\n",
    "$$\n",
    "\n",
    "Так как у нас несколько классов, далее используем **F1-macro** — среднее значение F1 по всем классам:\n",
    "\n",
    "$$\n",
    "F1_{\\text{macro}} = \\frac{1}{K} \\sum_{k=1}^{K} F1_k,\n",
    "$$\n",
    "\n",
    "где \\(K\\) — число классов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5e3a2-d92a-49c1-84d1-07f102ad0d09",
   "metadata": {},
   "source": [
    "**Создаём кросс-валидацию, чтобы в каждом фолде сохранялись пропорции классов**\n",
    "\n",
    "Для каждой модели собираем пайплайн вида:\n",
    "\n",
    "$$\n",
    "\\text{Pipeline: } X \\xrightarrow{\\text{preprocessor}} X_{\\text{transformed}} \\xrightarrow{\\text{clf}} \\hat{y}\n",
    "$$\n",
    "\n",
    "- считаем F1-macro через `cross_val_score`\n",
    "- выводим среднее и стандартное отклонение F1 по 5 фолдам для каждой модели `StratifiedKFold(n_splits=5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f8032-6157-42ea-9562-c91c16e63dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = \"f1_macro\"\n",
    "results = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocessor),\n",
    "        (\"clf\", clf)])\n",
    "    \n",
    "    # Оценка по 5-fold StratifiedKFold\n",
    "    scores = cross_val_score(\n",
    "        pipe,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv,\n",
    "        scoring=scoring)\n",
    "    results[name] = scores\n",
    "    print(f\"{name}: F1_macro = {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2725fe2-adf0-40f5-9d45-af6c0c206f88",
   "metadata": {},
   "source": [
    "Согласно построенному пайплайну предобработки по метрике **F1-macro**, все три рассмотренные модели показывают сопоставимое качество:\n",
    "- `LogisticRegression` ~ 0.61\n",
    "- `RandomForest` ~ 0.60\n",
    "- `GradientBoosting` ~ 0.62\n",
    "\n",
    "Наилучший средний F1-macro демонстрирует GradientBoosting. Поэтому выбираем эту модель для дальнейшего подбора гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8f7d2-4335-4fdf-8ec5-5bbc2d10ff4d",
   "metadata": {},
   "source": [
    "## 5. Подбор гиперпараметров для GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8cda0-66b8-4824-825f-38b8defa95a9",
   "metadata": {},
   "source": [
    "Реализуем GridSearchCV только для GradientBoosting, чтобы подобрать оптимальные гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88f65a-e1d8-491e-b78b-a084393ee3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),                     \n",
    "    (\"clf\", GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Сетка гиперпараметров для GradientBoosting\n",
    "param_grid_gb = {\n",
    "    \"clf__n_estimators\": [100, 200, 300],\n",
    "    \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"clf__max_depth\": [2, 3, 4],\n",
    "    \"clf__subsample\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(\n",
    "    estimator=gb_pipe,\n",
    "    param_grid=param_grid_gb,\n",
    "    cv=cv,           \n",
    "    scoring=scoring, \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "print(\"Лучшие параметры GradientBoosting:\", grid_search_gb.best_params_)\n",
    "print(\"Лучший F1_macro на CV:\", grid_search_gb.best_score_)\n",
    "\n",
    "# Сохраняем лучшую модель для блоков 6–7\n",
    "best_model = grid_search_gb.best_estimator_\n",
    "\n",
    "# Оценка на валидационной выборке\n",
    "y_valid_pred = best_model.predict(X_valid)\n",
    "print(\"\\nF1_macro на валидации:\", f1_score(y_valid, y_valid_pred, average=\"macro\"))\n",
    "print(\"\\nclassification_report:\")\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "print(\"\\nМатрица ошибок:\")\n",
    "print(confusion_matrix(y_valid, y_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58701d77-abaa-4f0b-8600-513df72700d6",
   "metadata": {},
   "source": [
    "По полученным результатам лучшие гиперпараметры:\n",
    "  `n_estimators = 200`, `learning_rate = 0.05`, `max_depth = 4`, `subsample = 0.8`\n",
    "\n",
    "* **Качество на кросс-валидации (train, 5-fold):** F1_macro = **0.63**\n",
    "\n",
    "* **Качество на отложенной валидации:**  F1_macro = **0.54**, accuracy = **0.85**\n",
    "\n",
    "  * класс 1 (Onshore): F1 = **0.91**, модель уверенно распознаёт сушу\n",
    "  * класс 0 (Offshore): F1 = **0.71**, часть объектов моря путается с сушей\n",
    "  * класс 2 (Onshore/Offshore): F1 = **0.00**, единичный объект не распознан, редкий класс фактически не моделируется\n",
    "\n",
    "В итоге, реализованный GradientBoosting хорошо разделяет основные два класса (0 и 1), но из-за дисбаланса по классу 2 метрика F1_macro на валидации проседает, так как было принято решение оставить все классы как есть."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068ad37-b781-460a-be16-a1dab2ccf1e1",
   "metadata": {},
   "source": [
    "## 6. Анализ важности признаков для GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c82b743-359f-42bc-b359-7ee33f9b70f0",
   "metadata": {},
   "source": [
    "Используем уже обученный пайплайн best_model:\n",
    "\n",
    "- `preprocess` - это наш preprocessor (StandardScaler + OneHotEncoder)\n",
    "\n",
    "- `clf` - настроенный GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288641a-a44c-4a1a-8d3f-4d0485a543c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем имена признаков\n",
    "ohe = best_model.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "cat_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "feature_names = np.concatenate([numeric_features_extended, cat_feature_names])\n",
    "\n",
    "# Важности признаков\n",
    "gb_clf = best_model.named_steps[\"clf\"]\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\"feature\": feature_names,\n",
    "                  \"importance\": gb_clf.feature_importances_})\n",
    "    .sort_values(by=\"importance\", ascending=False))\n",
    "display(feat_imp.head(20))\n",
    "\n",
    "# График топ-10\n",
    "top_n = 10\n",
    "feat_top = feat_imp.head(top_n).iloc[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feat_top[\"feature\"], feat_top[\"importance\"])\n",
    "plt.title(\"Top-10 наиболее важных признаков (GradientBoosting)\")\n",
    "plt.xlabel(\"Важность\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "for y, v in enumerate(feat_top[\"importance\"]):\n",
    "    plt.text(v, y, f\"{v:.3f}\", va=\"center\", ha=\"left\", fontsize=9)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e94f97-3312-4b23-9647-5cf3be5199aa",
   "metadata": {},
   "source": [
    "- **Географические признаки**  \n",
    "   Ведущие по важности: **Region_EUROPE** (0.16), **Latitude** (0.13), **Longitude** (0.12). Географическое положение месторождения (регион и координаты) являются ключевой фактор при разделении onshore/offshore\n",
    "\n",
    "- **Физические характеристики**  \n",
    "   В топ-10 входят **Thickness (net pay average ft)**, **Porosity**, **Permeability**, **Depth**, а также сконструированные признаки **net_to_gross** и **log_depth** (важностями  0.02–0.05). Это также отражает, что физические характеристики заметно влияют на тип месторождения\n",
    "\n",
    "- **Категориальные признаки**  \n",
    "   Важный вклад дают **Reservoir status_DEVELOPING**, отдельные категории **Tectonic regime** и **Basin name**. Статус разработки и тектоническая обстановка дополняют информацию о местоположении и физических параметрах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90011eb-65d2-4239-b87e-0d507f452abb",
   "metadata": {},
   "source": [
    "## 7. Обучение настроенного GradientBoosting на всём train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b52021-54e7-46c9-a386-1597da710f34",
   "metadata": {},
   "source": [
    "Используем лучшую модель, обученную на всех тренировочных данных (`X_full`, `y_full`), чтобы получить предсказания по тестовому набору и сформировать итоговый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319eec6e-912d-46ab-b150-411d15d83685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Готовим X_test \n",
    "X_test = test_prep[feature_cols].copy()\n",
    "X_test[\"net_to_gross\"] = X_test[\"Thickness (net pay average ft)\"] / X_test[\"Thickness (gross average ft)\"]\n",
    "X_test[\"log_depth\"] = np.log1p(X_test[\"Depth\"])\n",
    "\n",
    "# Переобучаем лучшую модель \n",
    "best_model.fit(X_full, y_full)\n",
    "\n",
    "# Предсказываем классы (0/1/2)\n",
    "test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Формируем отдельный csv-файл с результатми \n",
    "submission = pd.DataFrame({\n",
    "    \"index\": X_test.index,\n",
    "    \"Onshore/Offshore\": test_pred})\n",
    "display(submission.head())\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Файл submission.csv сохранён.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328cde32-9ad1-4ee1-871d-4b144ad697f1",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "Выполнен полный цикл решения задачи: EDA, предобработка, feature engineering и сравнение моделей по F1-macro. Лучшей оказалась настроенная GradientBoosting-модель, опирающаяся в основном на географические и физические признаки. На её основе получены предсказания для test и сформирован файл для Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3372d5-76bd-41b2-94ba-6f4049291b5f",
   "metadata": {},
   "source": [
    "Cкриншот строки с результатом из лидерборда Kaggle: https://disk.yandex.ru/i/M2EVZPbx6TYXWw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
